
model_type: annelid
architectures: [
    AnnelidLMModel
]

hidden_act: silu
initializer_range: 0.02
layer_norm_eps: 0.00001
partial_rotary_factor: 0.25
rope_theta: 10000
tie_word_embeddings: False
use_qkv_bias: True

hidden_size: 2048
intermediate_size: 5632

num_attention_heads: 32
num_hidden_layers: 24
num_key_value_heads: 32

max_position_embeddings: 1024

is_prefix_lm: False
is_quasi_lm: False
segment_size: 32
use_segment_embeds: False

_attn_implementation: eager
_gradient_checkpointing: True
use_cache: False
