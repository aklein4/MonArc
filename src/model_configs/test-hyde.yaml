
model_type: hyde
architectures: [
    HydeLmModel
]

hidden_act: silu
initializer_range: 0.02
layer_norm_eps: 0.00001
partial_rotary_factor: 0.25
rope_theta: 10000
tie_word_embeddings: False
use_qkv_bias: True

hidden_size: 256
intermediate_size: 128

num_attention_heads: 2
num_hidden_layers: 4
num_key_value_heads: 2

max_position_embeddings: 128

_attn_implementation: eager
gradient_checkpointing: False

attn_size: 128
input_factor: 128
output_factor: 128
